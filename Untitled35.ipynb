{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ingestion.py\n",
        "import requests\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "def get_oauth2_token(token_url: str, client_id: str, client_secret: str, scope: Optional[str]=None) -> str:\n",
        "    data = {\"grant_type\": \"client_credentials\"}\n",
        "    if scope:\n",
        "        data[\"scope\"] = scope\n",
        "    resp = requests.post(token_url, data=data, auth=(client_id, client_secret), timeout=10)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()[\"access_token\"]\n",
        "\n",
        "def fetch_api_oauth(api_url: str, token_url: str, client_id: str, client_secret: str, params: Dict = None) -> Dict[str,Any]:\n",
        "    token = get_oauth2_token(token_url, client_id, client_secret)\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\", \"Accept\": \"application/json\"}\n",
        "    r = requests.get(api_url, headers=headers, params=params, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def fetch_api_mtls(api_url: str, cert_tuple: tuple, ca_bundle: Optional[str] = None, params: Dict=None) -> Dict[str,Any]:\n",
        "    # cert_tuple = (\"/path/client.crt\", \"/path/client.key\")\n",
        "    verify = ca_bundle if ca_bundle else True\n",
        "    r = requests.get(api_url, cert=cert_tuple, verify=verify, params=params, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n"
      ],
      "metadata": {
        "id": "DmRPDKWBkYvm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize.py\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "CANONICAL_COLUMNS = [\n",
        "    \"train_number\",\"station_code\",\"timestamp\",\n",
        "    \"scheduled_arrival\",\"scheduled_departure\",\"actual_arrival\",\n",
        "    \"speed_kmph\",\"signal_aspect\",\"rolling_stock_health\",\n",
        "    \"weather_condition\",\"source\"\n",
        "]\n",
        "\n",
        "def parse_ts(x):\n",
        "    return pd.to_datetime(x, errors=\"coerce\")\n",
        "\n",
        "def normalize_timetable(timetable_json: List[Dict[str,Any]]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for r in timetable_json:\n",
        "        rows.append({\n",
        "            \"train_number\": str(r.get(\"train_number\") or r.get(\"train_no\") or \"\"),\n",
        "            \"station_code\": r.get(\"station_code\") or r.get(\"station\") or \"\",\n",
        "            \"timestamp\": parse_ts(r.get(\"last_update\") or r.get(\"timestamp\")),\n",
        "            \"scheduled_arrival\": parse_ts(r.get(\"scheduled_arrival\")),\n",
        "            \"scheduled_departure\": parse_ts(r.get(\"scheduled_departure\")),\n",
        "            \"actual_arrival\": parse_ts(r.get(\"actual_arrival\")),\n",
        "            \"speed_kmph\": r.get(\"speed_kmph\"),\n",
        "            \"signal_aspect\": None,\n",
        "            \"rolling_stock_health\": None,\n",
        "            \"weather_condition\": r.get(\"weather\"),\n",
        "            \"source\": \"timetable\"\n",
        "        })\n",
        "    return pd.DataFrame(rows)[CANONICAL_COLUMNS]\n",
        "\n",
        "def normalize_signalling(signalling_json: List[Dict[str,Any]]) -> pd.DataFrame:\n",
        "    rows=[]\n",
        "    for r in signalling_json:\n",
        "        rows.append({\n",
        "            \"train_number\": str(r.get(\"train_number\",\"\")),\n",
        "            \"station_code\": r.get(\"station_code\") or r.get(\"block_id\") or \"\",\n",
        "            \"timestamp\": parse_ts(r.get(\"timestamp\")),\n",
        "            \"scheduled_arrival\": None,\n",
        "            \"scheduled_departure\": None,\n",
        "            \"actual_arrival\": None,\n",
        "            \"speed_kmph\": r.get(\"approach_speed\"),\n",
        "            \"signal_aspect\": r.get(\"aspect\"),\n",
        "            \"rolling_stock_health\": None,\n",
        "            \"weather_condition\": r.get(\"weather\"),\n",
        "            \"source\": \"signalling\"\n",
        "        })\n",
        "    return pd.DataFrame(rows)[CANONICAL_COLUMNS]\n",
        "\n",
        "def normalize_rolling(rolling_json: List[Dict[str,Any]]) -> pd.DataFrame:\n",
        "    rows=[]\n",
        "    for r in rolling_json:\n",
        "        rows.append({\n",
        "            \"train_number\": str(r.get(\"train_number\",\"\")),\n",
        "            \"station_code\": r.get(\"nearest_station\") or \"\",\n",
        "            \"timestamp\": parse_ts(r.get(\"timestamp\")),\n",
        "            \"scheduled_arrival\": None,\n",
        "            \"scheduled_departure\": None,\n",
        "            \"actual_arrival\": parse_ts(r.get(\"actual_arrival\")),\n",
        "            \"speed_kmph\": r.get(\"speed\"),\n",
        "            \"signal_aspect\": None,\n",
        "            \"rolling_stock_health\": r.get(\"health_score\"),\n",
        "            \"weather_condition\": r.get(\"weather\"),\n",
        "            \"source\": \"rolling_stock\"\n",
        "        })\n",
        "    return pd.DataFrame(rows)[CANONICAL_COLUMNS]\n",
        "\n",
        "def merge_all(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
        "    df = pd.concat(dfs, ignore_index=True, sort=False)\n",
        "    for col in [\"timestamp\",\"scheduled_arrival\",\"scheduled_departure\",\"actual_arrival\"]:\n",
        "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Gd9TV_9pkdks"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features.py\n",
        "import pandas as pd\n",
        "import redis\n",
        "import json\n",
        "from typing import Optional\n",
        "\n",
        "def compute_labels_and_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Ensure scheduled & actual are present where available\n",
        "    df[\"delay_minutes\"] = (df[\"actual_arrival\"] - df[\"scheduled_arrival\"]).dt.total_seconds() / 60.0\n",
        "    df[\"delay_minutes\"] = df[\"delay_minutes\"].fillna(0.0)\n",
        "\n",
        "    # Sort per train to compute previous delays\n",
        "    df = df.sort_values([\"train_number\",\"scheduled_arrival\"]).reset_index(drop=True)\n",
        "\n",
        "    # prev_delay: last station delay for this train\n",
        "    df[\"prev_delay\"] = df.groupby(\"train_number\")[\"delay_minutes\"].shift(1).fillna(0.0)\n",
        "\n",
        "    # rolling average of last 3 delays (excluding current)\n",
        "    df[\"avg_prev3_delay\"] = df.groupby(\"train_number\")[\"delay_minutes\"].apply(\n",
        "        lambda s: s.shift(1).rolling(window=3, min_periods=1).mean()).fillna(0.0)\n",
        "\n",
        "    # temporal\n",
        "    df[\"minute_of_day\"] = df[\"scheduled_arrival\"].dt.hour*60 + df[\"scheduled_arrival\"].dt.minute\n",
        "    df[\"day_of_week\"] = df[\"scheduled_arrival\"].dt.dayofweek\n",
        "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5,6]).astype(int)\n",
        "\n",
        "    # fill numeric fields\n",
        "    df[\"speed_kmph\"] = pd.to_numeric(df[\"speed_kmph\"], errors=\"coerce\").fillna(df[\"speed_kmph\"].median())\n",
        "    df[\"rolling_stock_health\"] = pd.to_numeric(df[\"rolling_stock_health\"], errors=\"coerce\").fillna(df[\"rolling_stock_health\"].median())\n",
        "\n",
        "    # Target classification: delayed if > 5 minutes\n",
        "    df[\"delayed\"] = (df[\"delay_minutes\"] > 5.0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def push_online_features_to_redis(df: pd.DataFrame, redis_url: str=\"redis://localhost:6379/0\"):\n",
        "    r = redis.from_url(redis_url)\n",
        "    # For each train, push latest prev_delay and avg_prev3_delay\n",
        "    latest = df.sort_values(\"scheduled_arrival\").groupby(\"train_number\").tail(1)\n",
        "    for _, row in latest.iterrows():\n",
        "        key = f\"train:{row['train_number']}:online\"\n",
        "        payload = {\n",
        "            \"prev_delay\": float(row[\"prev_delay\"]),\n",
        "            \"avg_prev3_delay\": float(row[\"avg_prev3_delay\"]),\n",
        "            \"last_update\": str(row[\"scheduled_arrival\"])\n",
        "        }\n",
        "        r.set(key, json.dumps(payload))\n"
      ],
      "metadata": {
        "id": "BYlQJYi7kjcv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0958848b",
        "outputId": "ef6ec695-7823-453a-f5c5-7604ad562a0f"
      },
      "source": [
        "!pip install redis"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-6.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading redis-6.4.0-py3-none-any.whl (279 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/279.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m225.3/279.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-6.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUEg4JSCiB9N",
        "outputId": "016d0f71-56b7-4c0b-d345-0183f38ff2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's binary_error: 0.24575\tvalid_1's binary_error: 0.25\n",
            "Accuracy: 0.75\n",
            "F1 Score: 0.8571428571428571\n",
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/railway_records_large.csv\", parse_dates=[\"scheduled_arrival\",\"actual_arrival\"])\n",
        "\n",
        "# Target: delay > 5 min = delayed (1), else on-time (0)\n",
        "df[\"delay_minutes\"] = (df[\"actual_arrival\"] - df[\"scheduled_arrival\"]).dt.total_seconds()/60\n",
        "df[\"delayed\"] = (df[\"delay_minutes\"] > 5).astype(int)\n",
        "\n",
        "# Feature engineering\n",
        "df[\"minute_of_day\"] = df[\"scheduled_arrival\"].dt.hour*60 + df[\"scheduled_arrival\"].dt.minute\n",
        "df[\"day_of_week\"] = df[\"scheduled_arrival\"].dt.dayofweek\n",
        "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5,6]).astype(int)\n",
        "\n",
        "features = [\"minute_of_day\",\"day_of_week\",\"is_weekend\",\"speed_kmph\",\"rolling_stock_health\"]\n",
        "X = df[features]\n",
        "y = df[\"delayed\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Train LightGBM\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"binary_error\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 64,\n",
        "    \"verbose\": -1\n",
        "}\n",
        "model = lgb.train(params, train_data, valid_sets=[train_data, valid_data],\n",
        "                  num_boost_round=1000, callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
        "\n",
        "# Predictions\n",
        "y_pred = (model.predict(X_valid, num_iteration=model.best_iteration) > 0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "f1 = f1_score(y_valid, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Save the model and other artifacts\n",
        "model_artifacts = {\n",
        "    \"model_tuple\": (\"lgb\", model),\n",
        "    \"features\": features,\n",
        "    \"metrics\": {\"accuracy\": acc, \"f1_score\": f1, \"threshold\": 0.5} # Adding a threshold for the prediction\n",
        "}\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(\"./models\"):\n",
        "    os.makedirs(\"./models\")\n",
        "\n",
        "joblib.dump(model_artifacts, \"./models/railway_classifier.joblib\")\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# serve.py\n",
        "import joblib\n",
        "import numpy as np\n",
        "from fastapi import FastAPI, HTTPException, Depends\n",
        "from pydantic import BaseModel\n",
        "from datetime import datetime\n",
        "import redis, json\n",
        "import os\n",
        "\n",
        "MODEL_PATH = \"./models/railway_classifier.joblib\"\n",
        "\n",
        "app = FastAPI(title=\"Railway OnTime Classifier\")\n",
        "\n",
        "# Load the model and other artifacts\n",
        "try:\n",
        "    artifacts = joblib.load(MODEL_PATH)\n",
        "    model_tuple = artifacts[\"model_tuple\"]\n",
        "    # encoders = artifacts[\"encoders\"] # Encoders are not used in this version of serve.py\n",
        "    features = artifacts[\"features\"]\n",
        "    metrics = artifacts[\"metrics\"]\n",
        "except FileNotFoundError:\n",
        "    raise RuntimeError(f\"Model file not found at {MODEL_PATH}. Please train the model first.\")\n",
        "except KeyError as e:\n",
        "    raise RuntimeError(f\"Missing key in model artifacts: {e}. Ensure all required artifacts are saved.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error loading model artifacts: {e}\")\n",
        "\n",
        "\n",
        "# Redis client for online features\n",
        "r = redis.from_url(\"redis://localhost:6379/0\")\n",
        "\n",
        "# Simple token check (replace with OAuth2/JWT or mTLS in production)\n",
        "def verify_token(token: str):\n",
        "    # Replace with real verification: introspection, JWKS, or API gateway\n",
        "    if token != \"super-secret-token\":\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid token\")\n",
        "\n",
        "class PredictRequest(BaseModel):\n",
        "    train_number: str\n",
        "    station_code: str\n",
        "    scheduled_arrival: datetime\n",
        "    speed_kmph: float = None\n",
        "    rolling_stock_health: float = None\n",
        "    # optional: client should pass token in header; for demo we accept token param\n",
        "    token: str = None\n",
        "\n",
        "class PredictResponse(BaseModel):\n",
        "    delayed_probability: float\n",
        "    delayed: bool\n",
        "    threshold: float\n",
        "    explanation: dict\n",
        "\n",
        "def encode_with_le(val, le):\n",
        "    try:\n",
        "        return int(le.transform([str(val)])[0])\n",
        "    except Exception:\n",
        "        # unseen -> return median or 0\n",
        "        return 0\n",
        "\n",
        "def get_online_features(train_number: str):\n",
        "    key = f\"train:{train_number}:online\"\n",
        "    v = r.get(key)\n",
        "    if not v:\n",
        "        return {\"prev_delay\": 0.0, \"avg_prev3_delay\": 0.0}\n",
        "    try:\n",
        "        d = json.loads(v)\n",
        "        return {\"prev_delay\": float(d.get(\"prev_delay\",0.0)), \"avg_prev3_delay\": float(d.get(\"avg_prev3_delay\",0.0))}\n",
        "    except:\n",
        "        return {\"prev_delay\": 0.0, \"avg_prev3_delay\": 0.0}\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictResponse)\n",
        "def predict(req: PredictRequest):\n",
        "    # token check (in header or body). Replace this placeholder.\n",
        "    if req.token is None:\n",
        "        raise HTTPException(status_code=401, detail=\"Missing token\")\n",
        "    verify_token(req.token)\n",
        "\n",
        "    # Build features\n",
        "    minute_of_day = req.scheduled_arrival.hour*60 + req.scheduled_arrival.minute\n",
        "    day_of_week = req.scheduled_arrival.weekday()\n",
        "    is_weekend = int(day_of_week in (5,6))\n",
        "    speed = req.speed_kmph if req.speed_kmph is not None else 40.0\n",
        "    health = req.rolling_stock_health if req.rolling_stock_health is not None else 0.9\n",
        "\n",
        "    online = get_online_features(req.train_number)\n",
        "    prev_delay = online[\"prev_delay\"]\n",
        "    avg_prev3_delay = online[\"avg_prev3_delay\"]\n",
        "\n",
        "    # Removed encoding for train_number and station_code as they are not used in this model version\n",
        "    # train_le = encode_with_le(req.train_number, encoders[\"train_number\"])\n",
        "    # station_le = encode_with_le(req.station_code, encoders[\"station_code\"])\n",
        "\n",
        "    # Updated feature vector to match the trained model's features\n",
        "    vec = np.array([[minute_of_day, day_of_week, is_weekend, speed, health]])\n",
        "    kind, mdl = model_tuple\n",
        "    if kind == \"lgb\":\n",
        "        proba = mdl.predict(vec, num_iteration=mdl.best_iteration)[0]\n",
        "    else:\n",
        "        try:\n",
        "            proba = mdl.predict_proba(vec)[0][1]\n",
        "        except:\n",
        "            proba = float(mdl.predict(vec)[0])\n",
        "\n",
        "    threshold = metrics.get(\"threshold\", 0.5)\n",
        "    delayed = bool(proba > threshold)\n",
        "\n",
        "    # minimal explanation: return feature values; for full explanations integrate SHAP server-side\n",
        "    explanation = {\n",
        "        \"feature_values\": {\n",
        "            \"minute_of_day\": minute_of_day,\n",
        "            \"day_of_week\": day_of_week,\n",
        "            \"speed_kmph\": speed,\n",
        "            \"rolling_stock_health\": health,\n",
        "            \"prev_delay\": prev_delay, # Included online features in explanation\n",
        "            \"avg_prev3_delay\": avg_prev3_delay # Included online features in explanation\n",
        "        }\n",
        "    }\n",
        "    return PredictResponse(delayed_probability=float(proba), delayed=delayed, threshold=threshold, explanation=explanation)"
      ],
      "metadata": {
        "id": "lBUPNbePowms"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o94p9t0zqO6F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}